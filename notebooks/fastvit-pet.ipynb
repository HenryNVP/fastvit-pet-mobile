{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0cc5b57",
      "metadata": {
        "id": "f0cc5b57"
      },
      "source": [
        "# FASTVIT PET MOBILE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2480a7",
      "metadata": {
        "id": "ef2480a7"
      },
      "source": [
        "## 1. Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c57ee283",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c57ee283",
        "outputId": "95fc6d60-405a-404a-ba15-349ad2747e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastvit-pet-mobile'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 63 (delta 11), reused 57 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (63/63), 3.12 MiB | 37.63 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "/content/fastvit-pet-mobile\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HenryNVP/fastvit-pet-mobile.git\n",
        "%cd fastvit-pet-mobile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "92540182",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "92540182",
        "outputId": "cc2c3302-11d6-46b5-c7a0-4a003aa9a3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.8.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.0.22)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (11.3.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (6.0.3)\n",
            "Collecting onnxruntime (from -r requirements.txt (line 11))\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm->-r requirements.txt (line 5)) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->-r requirements.txt (line 5)) (0.6.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Collecting coloredlogs (from onnxruntime->-r requirements.txt (line 11))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 11)) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->-r requirements.txt (line 11)) (5.29.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->-r requirements.txt (line 11))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 5)) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 5)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 5)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 5)) (2025.10.5)\n",
            "Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a495b95",
      "metadata": {
        "id": "9a495b95"
      },
      "source": [
        "## 2. Download and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a06af4",
      "metadata": {
        "id": "a5a06af4"
      },
      "source": [
        "This downloads the official dataset and prepares 256x256 splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac2ae49d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ac2ae49d",
        "outputId": "5fc61dc2-5b09-420c-efbd-4840f2bbc9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading images.tar.gz from https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
            "/content/fastvit-pet-mobile/scripts/get_data.py:33: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(destination)\n",
            "Downloading annotations.tar.gz from https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
            "Prepared train=3680 val=1832 test=1837\n"
          ]
        }
      ],
      "source": [
        "!python scripts/get_data.py\n",
        "!python scripts/split_dataset.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1085f238",
      "metadata": {
        "id": "1085f238"
      },
      "source": [
        "## 3. Finetune teacher model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import fastvit.models\n",
        "from timm.models import create_model\n",
        "from fastvit.models.modules.mobileone import reparameterize_model\n",
        "\n",
        "# Create the model\n",
        "teacher_model = create_model(\"fastvit_t8\", pretrained=False)\n",
        "\n",
        "# Download and load checkpoint\n",
        "!wget -O fastvit_t8.pth.tar \\\n",
        "\"https://docs-assets.developer.apple.com/ml-research/models/fastvit/image_classification_models/fastvit_t8.pth.tar\"\n",
        "\n",
        "checkpoint = torch.load(\"fastvit_t8.pth.tar\", map_location='cpu')\n",
        "teacher_model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "# ... train ...\n",
        "\n",
        "# For inference\n",
        "teacher_model.eval()\n",
        "model_inf = reparameterize_model(teacher_model)\n",
        "# Use model_inf at test-time"
      ],
      "metadata": {
        "id": "yw8hxfy-SJn8",
        "outputId": "1dc504ea-4759-4ee3-e2c4-949263f5b4d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yw8hxfy-SJn8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-19 22:45:05--  https://docs-assets.developer.apple.com/ml-research/models/fastvit/image_classification_models/fastvit_t8.pth.tar\n",
            "Resolving docs-assets.developer.apple.com (docs-assets.developer.apple.com)... 17.253.118.202, 2403:300:a32:f100::2\n",
            "Connecting to docs-assets.developer.apple.com (docs-assets.developer.apple.com)|17.253.118.202|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16378213 (16M) [application/x-tar]\n",
            "Saving to: ‘fastvit_t8.pth.tar’\n",
            "\n",
            "fastvit_t8.pth.tar  100%[===================>]  15.62M  11.1MB/s    in 1.4s    \n",
            "\n",
            "2025-11-19 22:45:07 (11.1 MB/s) - ‘fastvit_t8.pth.tar’ saved [16378213/16378213]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load image\n",
        "img = Image.open(\"/content/fastvit-pet-mobile/data/train/Abyssinian/Abyssinian_1.jpg\").convert(\"RGB\")\n",
        "\n",
        "# Preprocess\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(img).unsqueeze(0)  # add batch dimension\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model_inf(input_tensor)  # shape: [1, num_classes]\n",
        "    predicted_class = torch.argmax(output, dim=1)\n",
        "    print(\"Predicted class index:\", predicted_class.item())"
      ],
      "metadata": {
        "id": "EeftaGi-Sxrj",
        "outputId": "765ffd99-4a79-4214-db37-8ab03baf20cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EeftaGi-Sxrj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: 273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python fastvit/train.py data --model fastvit_t8 -b 128 --lr 1e-3 --native-amp --mixup 0.2 --output ./output/teacher --input-size 3 256 256\n"
      ],
      "metadata": {
        "id": "-YnLKNuVTF3b",
        "outputId": "19a7e4a0-f024-4b32-a249-ae6db8c82992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-YnLKNuVTF3b",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "If for semantic segmentation, please install mmsegmentation first\n",
            "If for detection, please install mmdetection first\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:956: UserWarning: Overwriting fastvit_t8 in registry with fastvit.models.fastvit.fastvit_t8. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:978: UserWarning: Overwriting fastvit_t12 in registry with fastvit.models.fastvit.fastvit_t12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:1000: UserWarning: Overwriting fastvit_s12 in registry with fastvit.models.fastvit.fastvit_s12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:1022: UserWarning: Overwriting fastvit_sa12 in registry with fastvit.models.fastvit.fastvit_sa12. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:1046: UserWarning: Overwriting fastvit_sa24 in registry with fastvit.models.fastvit.fastvit_sa24. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:1070: UserWarning: Overwriting fastvit_sa36 in registry with fastvit.models.fastvit.fastvit_sa36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/fastvit-pet-mobile/fastvit/models/fastvit.py:1095: UserWarning: Overwriting fastvit_ma36 in registry with fastvit.models.fastvit.fastvit_ma36. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Training with a single process on 1 GPUs.\n",
            "Model fastvit_t8 created, param count:4026232\n",
            "Data processing configuration for current model + dataset:\n",
            "\tinput_size: (3, 256, 256)\n",
            "\tinterpolation: bicubic\n",
            "\tmean: (0.485, 0.456, 0.406)\n",
            "\tstd: (0.229, 0.224, 0.225)\n",
            "\tcrop_pct: 0.875\n",
            "\tcrop_mode: center\n",
            "Using native Torch AMP. Training in mixed precision.\n",
            "Scheduled epochs: 310\n",
            "/content/fastvit-pet-mobile/fastvit/train.py:1399: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp_autocast():\n",
            "Train: 0 [   0/28 (  0%)]  Loss: 6.920 (6.92)  Time: 74.260s,    1.72/s  (74.260s,    1.72/s)  LR: 1.000e-06, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.269 (1.269)\n",
            "Train: 0 [  27/28 (100%)]  Loss: 6.910 (6.91)  Time: 0.357s,  358.22/s  (3.006s,   42.58/s)  LR: 1.000e-06, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.058)\n",
            "/content/fastvit-pet-mobile/fastvit/train.py:1515: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp_autocast():\n",
            "Test: [   0/14]  Time: 0.862 (0.862)  Loss:  6.9453 (6.9453)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test: [  14/14]  Time: 4.316 (0.445)  Loss:  6.9570 (6.9164)  Acc@1:  0.0000 ( 0.4367)  Acc@5:  0.0000 ( 2.2380)\n",
            "Test (EMA): [   0/14]  Time: 0.537 (0.537)  Loss:  6.9102 (6.9102)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.140)  Loss:  6.9102 (6.9077)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.3275)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            "\n",
            "Train: 1 [   0/28 (  0%)]  Loss: 6.898 (6.90)  Time: 1.412s,   90.68/s  (1.412s,   90.68/s)  LR: 2.008e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.053 (1.053)\n",
            "Train: 1 [  27/28 (100%)]  Loss: 5.971 (6.46)  Time: 0.359s,  356.95/s  (0.407s,  314.67/s)  LR: 2.008e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.050)\n",
            "Test: [   0/14]  Time: 0.755 (0.755)  Loss:  6.1133 (6.1133)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  2.3438 ( 2.3438)\n",
            "Test: [  14/14]  Time: 0.025 (0.151)  Loss:  6.0508 (6.0513)  Acc@1:  0.0000 ( 4.4760)  Acc@5:  0.0000 (17.3581)\n",
            "Test (EMA): [   0/14]  Time: 0.744 (0.744)  Loss:  6.9102 (6.9102)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.151)  Loss:  6.9102 (6.9080)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.9279)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 2 [   0/28 (  0%)]  Loss: 5.926 (5.93)  Time: 1.324s,   96.65/s  (1.324s,   96.65/s)  LR: 4.006e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.953 (0.953)\n",
            "Train: 2 [  27/28 (100%)]  Loss: 4.333 (4.83)  Time: 0.360s,  355.39/s  (0.408s,  313.97/s)  LR: 4.006e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.047)\n",
            "Test: [   0/14]  Time: 0.521 (0.521)  Loss:  3.6934 (3.6934)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 14.8438 (14.8438)\n",
            "Test: [  14/14]  Time: 0.025 (0.139)  Loss:  3.5039 (3.6808)  Acc@1:  7.5000 ( 5.6223)  Acc@5: 45.0000 (21.3428)\n",
            "Test (EMA): [   0/14]  Time: 0.759 (0.759)  Loss:  6.9062 (6.9062)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.153)  Loss:  6.9102 (6.9072)  Acc@1:  0.0000 ( 0.1638)  Acc@5:  0.0000 ( 2.4563)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 3 [   0/28 (  0%)]  Loss: 4.321 (4.32)  Time: 1.326s,   96.51/s  (1.326s,   96.51/s)  LR: 6.004e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.936 (0.936)\n",
            "Train: 3 [  27/28 (100%)]  Loss: 4.223 (4.25)  Time: 0.362s,  353.11/s  (0.409s,  313.06/s)  LR: 6.004e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.046)\n",
            "Test: [   0/14]  Time: 0.749 (0.749)  Loss:  3.7598 (3.7598)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  6.2500 ( 6.2500)\n",
            "Test: [  14/14]  Time: 0.025 (0.152)  Loss:  3.5449 (3.6157)  Acc@1:  2.5000 ( 7.7511)  Acc@5: 32.5000 (27.9476)\n",
            "Test (EMA): [   0/14]  Time: 0.777 (0.777)  Loss:  6.9062 (6.9062)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.9102 (6.9069)  Acc@1:  0.0000 ( 2.0742)  Acc@5:  0.0000 ( 2.6747)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 4 [   0/28 (  0%)]  Loss: 4.209 (4.21)  Time: 1.409s,   90.86/s  (1.409s,   90.86/s)  LR: 8.002e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.048 (1.048)\n",
            "Train: 4 [  27/28 (100%)]  Loss: 4.204 (4.22)  Time: 0.363s,  352.56/s  (0.413s,  310.18/s)  LR: 8.002e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.050)\n",
            "Test: [   0/14]  Time: 0.773 (0.773)  Loss:  3.6328 (3.6328)  Acc@1: 10.1562 (10.1562)  Acc@5: 34.3750 (34.3750)\n",
            "Test: [  14/14]  Time: 0.025 (0.153)  Loss:  3.3379 (3.5786)  Acc@1: 25.0000 ( 5.9498)  Acc@5: 65.0000 (28.1114)\n",
            "Test (EMA): [   0/14]  Time: 0.535 (0.535)  Loss:  6.9062 (6.9062)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.145)  Loss:  6.9062 (6.9062)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 ( 2.7293)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 5 [   0/28 (  0%)]  Loss: 4.172 (4.17)  Time: 1.497s,   85.49/s  (1.497s,   85.49/s)  LR: 9.993e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.121 (1.121)\n",
            "Train: 5 [  27/28 (100%)]  Loss: 4.206 (4.19)  Time: 0.377s,  339.83/s  (0.417s,  306.94/s)  LR: 9.993e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.053)\n",
            "Test: [   0/14]  Time: 0.766 (0.766)  Loss:  3.4570 (3.4570)  Acc@1: 10.9375 (10.9375)  Acc@5: 43.7500 (43.7500)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  3.7168 (3.5229)  Acc@1:  5.0000 ( 6.3865)  Acc@5: 17.5000 (32.0961)\n",
            "Test (EMA): [   0/14]  Time: 0.762 (0.762)  Loss:  6.9062 (6.9062)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.153)  Loss:  6.9062 (6.9054)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 ( 3.0022)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 6 [   0/28 (  0%)]  Loss: 4.152 (4.15)  Time: 1.411s,   90.70/s  (1.411s,   90.70/s)  LR: 9.990e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.044 (1.044)\n",
            "Train: 6 [  27/28 (100%)]  Loss: 4.244 (4.17)  Time: 0.366s,  350.00/s  (0.416s,  307.49/s)  LR: 9.990e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.050)\n",
            "Test: [   0/14]  Time: 0.770 (0.770)  Loss:  3.5605 (3.5605)  Acc@1:  0.0000 ( 0.0000)  Acc@5: 23.4375 (23.4375)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  3.2617 (3.4836)  Acc@1:  2.5000 ( 9.5524)  Acc@5: 47.5000 (32.4782)\n",
            "Test (EMA): [   0/14]  Time: 0.762 (0.762)  Loss:  6.9062 (6.9062)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.153)  Loss:  6.9062 (6.9038)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 ( 5.5131)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 7 [   0/28 (  0%)]  Loss: 4.073 (4.07)  Time: 1.123s,  114.03/s  (1.123s,  114.03/s)  LR: 9.987e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.744 (0.744)\n",
            "Train: 7 [  27/28 (100%)]  Loss: 4.244 (4.16)  Time: 0.369s,  347.22/s  (0.408s,  313.53/s)  LR: 9.987e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.039)\n",
            "Test: [   0/14]  Time: 0.780 (0.780)  Loss:  3.6094 (3.6094)  Acc@1: 10.9375 (10.9375)  Acc@5: 24.2188 (24.2188)\n",
            "Test: [  14/14]  Time: 0.026 (0.155)  Loss:  3.3125 (3.4249)  Acc@1:  7.5000 ( 9.6070)  Acc@5: 50.0000 (35.3712)\n",
            "Test (EMA): [   0/14]  Time: 0.789 (0.789)  Loss:  6.9023 (6.9023)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.156)  Loss:  6.9062 (6.9024)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 ( 8.1332)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 8 [   0/28 (  0%)]  Loss: 4.047 (4.05)  Time: 1.417s,   90.34/s  (1.417s,   90.34/s)  LR: 9.983e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.039 (1.039)\n",
            "Train: 8 [  27/28 (100%)]  Loss: 4.060 (4.15)  Time: 0.370s,  346.21/s  (0.416s,  307.36/s)  LR: 9.983e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.049)\n",
            "Test: [   0/14]  Time: 0.597 (0.597)  Loss:  3.4512 (3.4512)  Acc@1:  0.7812 ( 0.7812)  Acc@5: 27.3438 (27.3438)\n",
            "Test: [  14/14]  Time: 0.026 (0.143)  Loss:  3.1836 (3.3696)  Acc@1:  5.0000 (11.5175)  Acc@5: 50.0000 (38.9192)\n",
            "Test (EMA): [   0/14]  Time: 0.532 (0.532)  Loss:  6.9023 (6.9023)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.144)  Loss:  6.9023 (6.9013)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (10.2074)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 9 [   0/28 (  0%)]  Loss: 4.113 (4.11)  Time: 1.132s,  113.11/s  (1.132s,  113.11/s)  LR: 9.978e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.752 (0.752)\n",
            "Train: 9 [  27/28 (100%)]  Loss: 3.996 (4.12)  Time: 0.366s,  350.11/s  (0.407s,  314.19/s)  LR: 9.978e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.040)\n",
            "Test: [   0/14]  Time: 0.783 (0.783)  Loss:  3.4785 (3.4785)  Acc@1:  6.2500 ( 6.2500)  Acc@5: 31.2500 (31.2500)\n",
            "Test: [  14/14]  Time: 0.026 (0.155)  Loss:  3.0547 (3.3254)  Acc@1:  5.0000 (12.6092)  Acc@5: 57.5000 (39.1921)\n",
            "Test (EMA): [   0/14]  Time: 0.775 (0.775)  Loss:  6.8984 (6.8984)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.154)  Loss:  6.9023 (6.8988)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (10.7533)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-1.pth.tar', 0.0)\n",
            "\n",
            "Train: 10 [   0/28 (  0%)]  Loss: 4.070 (4.07)  Time: 1.355s,   94.46/s  (1.355s,   94.46/s)  LR: 9.973e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.989 (0.989)\n",
            "Train: 10 [  27/28 (100%)]  Loss: 4.099 (4.10)  Time: 0.364s,  351.28/s  (0.414s,  308.92/s)  LR: 9.973e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.048)\n",
            "Test: [   0/14]  Time: 0.769 (0.769)  Loss:  3.2871 (3.2871)  Acc@1: 11.7188 (11.7188)  Acc@5: 42.1875 (42.1875)\n",
            "Test: [  14/14]  Time: 0.025 (0.154)  Loss:  2.9766 (3.3206)  Acc@1: 17.5000 (13.2642)  Acc@5: 62.5000 (41.4301)\n",
            "Test (EMA): [   0/14]  Time: 0.746 (0.746)  Loss:  6.8984 (6.8984)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.152)  Loss:  6.8984 (6.8965)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (10.8624)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-10.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-0.pth.tar', 0.0)\n",
            "\n",
            "Train: 11 [   0/28 (  0%)]  Loss: 4.017 (4.02)  Time: 1.109s,  115.37/s  (1.109s,  115.37/s)  LR: 9.967e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.733 (0.733)\n",
            "Train: 11 [  27/28 (100%)]  Loss: 4.054 (4.07)  Time: 0.364s,  351.24/s  (0.413s,  309.90/s)  LR: 9.967e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.039)\n",
            "Test: [   0/14]  Time: 0.535 (0.535)  Loss:  3.4609 (3.4609)  Acc@1: 10.1562 (10.1562)  Acc@5: 38.2812 (38.2812)\n",
            "Test: [  14/14]  Time: 0.026 (0.146)  Loss:  3.7500 (3.2777)  Acc@1:  0.0000 (13.2096)  Acc@5: 12.5000 (44.4323)\n",
            "Test (EMA): [   0/14]  Time: 0.753 (0.753)  Loss:  6.8945 (6.8945)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.152)  Loss:  6.8984 (6.8943)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (10.9170)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-10.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-11.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-2.pth.tar', 0.16375545851528384)\n",
            "\n",
            "Train: 12 [   0/28 (  0%)]  Loss: 4.167 (4.17)  Time: 1.062s,  120.49/s  (1.062s,  120.49/s)  LR: 9.961e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.685 (0.685)\n",
            "Train: 12 [  27/28 (100%)]  Loss: 3.956 (4.06)  Time: 0.367s,  348.80/s  (0.407s,  314.21/s)  LR: 9.961e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.041)\n",
            "Test: [   0/14]  Time: 0.753 (0.753)  Loss:  4.0078 (4.0078)  Acc@1:  0.7812 ( 0.7812)  Acc@5: 10.9375 (10.9375)\n",
            "Test: [  14/14]  Time: 0.025 (0.153)  Loss:  3.0547 (3.3009)  Acc@1: 10.0000 (11.6812)  Acc@5: 52.5000 (40.3384)\n",
            "Test (EMA): [   0/14]  Time: 0.732 (0.732)  Loss:  6.8906 (6.8906)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.151)  Loss:  6.8945 (6.8913)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (10.9170)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-10.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-11.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-12.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-3.pth.tar', 2.074235807860262)\n",
            "\n",
            "Train: 13 [   0/28 (  0%)]  Loss: 4.049 (4.05)  Time: 1.487s,   86.07/s  (1.487s,   86.07/s)  LR: 9.954e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.122 (1.122)\n",
            "Train: 13 [  27/28 (100%)]  Loss: 4.124 (4.05)  Time: 0.369s,  346.48/s  (0.420s,  304.52/s)  LR: 9.954e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.053)\n",
            "Test: [   0/14]  Time: 0.555 (0.555)  Loss:  3.5215 (3.5215)  Acc@1:  7.0312 ( 7.0312)  Acc@5: 24.2188 (24.2188)\n",
            "Test: [  14/14]  Time: 0.026 (0.140)  Loss:  3.3242 (3.2241)  Acc@1: 12.5000 (14.7926)  Acc@5: 47.5000 (47.0524)\n",
            "Test (EMA): [   0/14]  Time: 0.700 (0.700)  Loss:  6.8906 (6.8906)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.149)  Loss:  6.8906 (6.8884)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (10.9170)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-10.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-11.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-12.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-13.pth.tar', 2.7292576419213974)\n",
            "\n",
            "Train: 14 [   0/28 (  0%)]  Loss: 4.106 (4.11)  Time: 1.090s,  117.41/s  (1.090s,  117.41/s)  LR: 9.947e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.709 (0.709)\n",
            "Train: 14 [  27/28 (100%)]  Loss: 4.081 (4.04)  Time: 0.368s,  347.39/s  (0.405s,  315.99/s)  LR: 9.947e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.038)\n",
            "Test: [   0/14]  Time: 0.769 (0.769)  Loss:  3.2852 (3.2852)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 42.9688 (42.9688)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  2.9551 (3.1180)  Acc@1: 27.5000 (16.9214)  Acc@5: 62.5000 (49.0175)\n",
            "Test (EMA): [   0/14]  Time: 0.742 (0.742)  Loss:  6.8867 (6.8867)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.152)  Loss:  6.8906 (6.8857)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (11.5721)\n",
            "Train: 15 [   0/28 (  0%)]  Loss: 4.134 (4.13)  Time: 1.559s,   82.10/s  (1.559s,   82.10/s)  LR: 9.939e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.194 (1.194)\n",
            "Train: 15 [  27/28 (100%)]  Loss: 4.085 (4.03)  Time: 0.368s,  347.86/s  (0.422s,  303.51/s)  LR: 9.939e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.055)\n",
            "Test: [   0/14]  Time: 0.541 (0.541)  Loss:  3.5977 (3.5977)  Acc@1:  6.2500 ( 6.2500)  Acc@5: 28.9062 (28.9062)\n",
            "Test: [  14/14]  Time: 0.026 (0.146)  Loss:  3.7227 (3.1408)  Acc@1:  2.5000 (15.3384)  Acc@5: 27.5000 (48.6900)\n",
            "Test (EMA): [   0/14]  Time: 0.784 (0.784)  Loss:  6.8828 (6.8828)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.155)  Loss:  6.8867 (6.8818)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (11.8450)\n",
            "Train: 16 [   0/28 (  0%)]  Loss: 4.028 (4.03)  Time: 1.461s,   87.63/s  (1.461s,   87.63/s)  LR: 9.931e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.095 (1.095)\n",
            "Train: 16 [  27/28 (100%)]  Loss: 4.005 (4.00)  Time: 0.366s,  349.93/s  (0.418s,  306.47/s)  LR: 9.931e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.052)\n",
            "Test: [   0/14]  Time: 0.553 (0.553)  Loss:  3.2520 (3.2520)  Acc@1: 10.9375 (10.9375)  Acc@5: 40.6250 (40.6250)\n",
            "Test: [  14/14]  Time: 0.025 (0.145)  Loss:  3.1348 (3.1035)  Acc@1: 25.0000 (18.6135)  Acc@5: 55.0000 (52.1288)\n",
            "Test (EMA): [   0/14]  Time: 0.763 (0.763)  Loss:  6.8789 (6.8789)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.153)  Loss:  6.8828 (6.8771)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (13.5371)\n",
            "Train: 17 [   0/28 (  0%)]  Loss: 3.980 (3.98)  Time: 1.446s,   88.51/s  (1.446s,   88.51/s)  LR: 9.922e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.080 (1.080)\n",
            "Train: 17 [  27/28 (100%)]  Loss: 3.941 (4.00)  Time: 0.367s,  348.75/s  (0.418s,  306.39/s)  LR: 9.922e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.051)\n",
            "Test: [   0/14]  Time: 0.697 (0.697)  Loss:  3.3965 (3.3965)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 42.1875 (42.1875)\n",
            "Test: [  14/14]  Time: 0.026 (0.149)  Loss:  3.6660 (3.0951)  Acc@1: 15.0000 (17.0306)  Acc@5: 30.0000 (52.0742)\n",
            "Test (EMA): [   0/14]  Time: 0.767 (0.767)  Loss:  6.8750 (6.8750)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.8789 (6.8732)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (13.3734)\n",
            "Train: 18 [   0/28 (  0%)]  Loss: 3.947 (3.95)  Time: 1.356s,   94.36/s  (1.356s,   94.36/s)  LR: 9.912e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.978 (0.978)\n",
            "Train: 18 [  27/28 (100%)]  Loss: 4.163 (3.98)  Time: 0.369s,  347.24/s  (0.414s,  309.27/s)  LR: 9.912e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.047)\n",
            "Test: [   0/14]  Time: 0.772 (0.772)  Loss:  3.4297 (3.4297)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 36.7188 (36.7188)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  3.5566 (3.0628)  Acc@1:  5.0000 (17.9585)  Acc@5: 32.5000 (52.5109)\n",
            "Test (EMA): [   0/14]  Time: 0.536 (0.536)  Loss:  6.8672 (6.8672)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.144)  Loss:  6.8750 (6.8682)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (13.2642)\n",
            "Train: 19 [   0/28 (  0%)]  Loss: 4.102 (4.10)  Time: 1.344s,   95.21/s  (1.344s,   95.21/s)  LR: 9.902e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.979 (0.979)\n",
            "Train: 19 [  27/28 (100%)]  Loss: 3.772 (3.99)  Time: 0.369s,  346.49/s  (0.415s,  308.41/s)  LR: 9.902e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.048)\n",
            "Test: [   0/14]  Time: 0.534 (0.534)  Loss:  3.2246 (3.2246)  Acc@1: 10.9375 (10.9375)  Acc@5: 46.8750 (46.8750)\n",
            "Test: [  14/14]  Time: 0.026 (0.145)  Loss:  3.1309 (2.9301)  Acc@1: 20.0000 (21.1245)  Acc@5: 50.0000 (59.0611)\n",
            "Test (EMA): [   0/14]  Time: 0.767 (0.767)  Loss:  6.8633 (6.8633)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.154)  Loss:  6.8672 (6.8628)  Acc@1:  0.0000 ( 2.7293)  Acc@5:  0.0000 (13.5371)\n",
            "Train: 20 [   0/28 (  0%)]  Loss: 3.812 (3.81)  Time: 1.107s,  115.66/s  (1.107s,  115.66/s)  LR: 9.892e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.727 (0.727)\n",
            "Train: 20 [  27/28 (100%)]  Loss: 3.750 (3.95)  Time: 0.367s,  348.93/s  (0.406s,  315.38/s)  LR: 9.892e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.039)\n",
            "Test: [   0/14]  Time: 0.766 (0.766)  Loss:  3.0781 (3.0781)  Acc@1: 11.7188 (11.7188)  Acc@5: 50.0000 (50.0000)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  3.1387 (2.9207)  Acc@1: 15.0000 (20.3603)  Acc@5: 42.5000 (58.7336)\n",
            "Test (EMA): [   0/14]  Time: 0.755 (0.755)  Loss:  6.8594 (6.8594)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.153)  Loss:  6.8633 (6.8575)  Acc@1:  0.0000 ( 2.7838)  Acc@5:  0.0000 (13.6463)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-20.pth.tar', 2.7838427947598254)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-10.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-11.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-12.pth.tar', 2.7292576419213974)\n",
            "\n",
            "Train: 21 [   0/28 (  0%)]  Loss: 3.941 (3.94)  Time: 1.031s,  124.10/s  (1.031s,  124.10/s)  LR: 9.881e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.653 (0.653)\n",
            "Train: 21 [  27/28 (100%)]  Loss: 3.757 (3.94)  Time: 0.368s,  348.27/s  (0.404s,  316.54/s)  LR: 9.881e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.036)\n",
            "Test: [   0/14]  Time: 0.769 (0.769)  Loss:  3.2617 (3.2617)  Acc@1: 10.1562 (10.1562)  Acc@5: 44.5312 (44.5312)\n",
            "Test: [  14/14]  Time: 0.025 (0.154)  Loss:  3.0117 (2.9375)  Acc@1: 17.5000 (21.3428)  Acc@5: 45.0000 (56.5502)\n",
            "Test (EMA): [   0/14]  Time: 0.714 (0.714)  Loss:  6.8516 (6.8516)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.150)  Loss:  6.8594 (6.8515)  Acc@1:  0.0000 ( 3.0022)  Acc@5:  0.0000 (13.7009)\n",
            "Current checkpoints:\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-21.pth.tar', 3.002183406113537)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-20.pth.tar', 2.7838427947598254)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-4.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-5.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-6.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-7.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-8.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-9.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-10.pth.tar', 2.7292576419213974)\n",
            " ('./output/teacher/20251119-225323-fastvit_t8-256/checkpoint-11.pth.tar', 2.7292576419213974)\n",
            "\n",
            "Train: 22 [   0/28 (  0%)]  Loss: 4.159 (4.16)  Time: 1.419s,   90.21/s  (1.419s,   90.21/s)  LR: 9.869e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.052 (1.052)\n",
            "Train: 22 [  27/28 (100%)]  Loss: 4.089 (3.90)  Time: 0.369s,  347.02/s  (0.418s,  306.47/s)  LR: 9.869e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.050)\n",
            "Test: [   0/14]  Time: 0.765 (0.765)  Loss:  3.2344 (3.2344)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 47.6562 (47.6562)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  3.2441 (2.8593)  Acc@1:  7.5000 (21.9432)  Acc@5: 40.0000 (58.6790)\n",
            "Test (EMA): [   0/14]  Time: 0.540 (0.540)  Loss:  6.8477 (6.8477)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.146)  Loss:  6.8516 (6.8456)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.7555)\n",
            "Train: 23 [   0/28 (  0%)]  Loss: 3.694 (3.69)  Time: 1.372s,   93.30/s  (1.372s,   93.30/s)  LR: 9.857e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.994 (0.994)\n",
            "Train: 23 [  27/28 (100%)]  Loss: 3.882 (3.90)  Time: 0.369s,  346.81/s  (0.416s,  307.92/s)  LR: 9.857e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.048)\n",
            "Test: [   0/14]  Time: 0.775 (0.775)  Loss:  3.3262 (3.3262)  Acc@1:  9.3750 ( 9.3750)  Acc@5: 45.3125 (45.3125)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  2.7031 (2.8715)  Acc@1: 37.5000 (21.5611)  Acc@5: 65.0000 (59.3341)\n",
            "Test (EMA): [   0/14]  Time: 0.713 (0.713)  Loss:  6.8398 (6.8398)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.150)  Loss:  6.8438 (6.8383)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.7555)\n",
            "Train: 24 [   0/28 (  0%)]  Loss: 3.632 (3.63)  Time: 1.119s,  114.42/s  (1.119s,  114.42/s)  LR: 9.844e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.741 (0.741)\n",
            "Train: 24 [  27/28 (100%)]  Loss: 3.832 (3.93)  Time: 0.368s,  347.77/s  (0.407s,  314.36/s)  LR: 9.844e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.039)\n",
            "Test: [   0/14]  Time: 0.529 (0.529)  Loss:  3.2617 (3.2617)  Acc@1: 10.1562 (10.1562)  Acc@5: 39.8438 (39.8438)\n",
            "Test: [  14/14]  Time: 0.025 (0.146)  Loss:  2.6680 (2.8759)  Acc@1: 30.0000 (21.7795)  Acc@5: 65.0000 (58.2969)\n",
            "Test (EMA): [   0/14]  Time: 0.580 (0.580)  Loss:  6.8320 (6.8320)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.146)  Loss:  6.8359 (6.8316)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.7009)\n",
            "Train: 25 [   0/28 (  0%)]  Loss: 3.734 (3.73)  Time: 1.443s,   88.69/s  (1.443s,   88.69/s)  LR: 9.831e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.076 (1.076)\n",
            "Train: 25 [  27/28 (100%)]  Loss: 4.099 (3.83)  Time: 0.366s,  349.63/s  (0.417s,  306.69/s)  LR: 9.831e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.051)\n",
            "Test: [   0/14]  Time: 0.743 (0.743)  Loss:  3.3047 (3.3047)  Acc@1: 10.9375 (10.9375)  Acc@5: 45.3125 (45.3125)\n",
            "Test: [  14/14]  Time: 0.026 (0.152)  Loss:  4.0898 (2.9460)  Acc@1:  0.0000 (20.6878)  Acc@5: 15.0000 (55.6769)\n",
            "Test (EMA): [   0/14]  Time: 0.779 (0.779)  Loss:  6.8281 (6.8281)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.8281 (6.8251)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.7009)\n",
            "Train: 26 [   0/28 (  0%)]  Loss: 4.001 (4.00)  Time: 1.490s,   85.92/s  (1.490s,   85.92/s)  LR: 9.818e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.113 (1.113)\n",
            "Train: 26 [  27/28 (100%)]  Loss: 3.714 (3.85)  Time: 0.368s,  348.25/s  (0.419s,  305.34/s)  LR: 9.818e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.052)\n",
            "Test: [   0/14]  Time: 0.761 (0.761)  Loss:  3.0742 (3.0742)  Acc@1: 15.6250 (15.6250)  Acc@5: 54.6875 (54.6875)\n",
            "Test: [  14/14]  Time: 0.026 (0.153)  Loss:  3.5879 (2.8836)  Acc@1: 10.0000 (21.9432)  Acc@5: 32.5000 (58.7336)\n",
            "Test (EMA): [   0/14]  Time: 0.744 (0.744)  Loss:  6.8164 (6.8164)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.152)  Loss:  6.8203 (6.8168)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.6463)\n",
            "Train: 27 [   0/28 (  0%)]  Loss: 3.826 (3.83)  Time: 1.454s,   88.01/s  (1.454s,   88.01/s)  LR: 9.803e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.084 (1.084)\n",
            "Train: 27 [  27/28 (100%)]  Loss: 3.897 (3.82)  Time: 0.365s,  350.80/s  (0.423s,  302.84/s)  LR: 9.803e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.051)\n",
            "Test: [   0/14]  Time: 0.768 (0.768)  Loss:  2.8359 (2.8359)  Acc@1: 29.6875 (29.6875)  Acc@5: 61.7188 (61.7188)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  2.9746 (2.7159)  Acc@1: 20.0000 (25.6550)  Acc@5: 50.0000 (63.3188)\n",
            "Test (EMA): [   0/14]  Time: 0.770 (0.770)  Loss:  6.8086 (6.8086)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.8125 (6.8081)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.5917)\n",
            "Train: 28 [   0/28 (  0%)]  Loss: 3.492 (3.49)  Time: 1.500s,   85.32/s  (1.500s,   85.32/s)  LR: 9.789e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.120 (1.120)\n",
            "Train: 28 [  27/28 (100%)]  Loss: 3.566 (3.82)  Time: 0.367s,  348.46/s  (0.420s,  304.92/s)  LR: 9.789e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.053)\n",
            "Test: [   0/14]  Time: 0.775 (0.775)  Loss:  3.1211 (3.1211)  Acc@1:  8.5938 ( 8.5938)  Acc@5: 50.7812 (50.7812)\n",
            "Test: [  14/14]  Time: 0.025 (0.155)  Loss:  2.6387 (2.6983)  Acc@1: 27.5000 (25.7096)  Acc@5: 67.5000 (62.9913)\n",
            "Test (EMA): [   0/14]  Time: 0.776 (0.776)  Loss:  6.8008 (6.8008)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.8047 (6.7992)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.6463)\n",
            "Train: 29 [   0/28 (  0%)]  Loss: 3.969 (3.97)  Time: 1.005s,  127.35/s  (1.005s,  127.35/s)  LR: 9.773e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.626 (0.626)\n",
            "Train: 29 [  27/28 (100%)]  Loss: 3.594 (3.79)  Time: 0.367s,  348.59/s  (0.409s,  312.74/s)  LR: 9.773e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.041)\n",
            "Test: [   0/14]  Time: 0.532 (0.532)  Loss:  3.0469 (3.0469)  Acc@1: 17.1875 (17.1875)  Acc@5: 53.9062 (53.9062)\n",
            "Test: [  14/14]  Time: 0.025 (0.139)  Loss:  2.1055 (2.6333)  Acc@1: 50.0000 (27.6201)  Acc@5: 80.0000 (65.6659)\n",
            "Test (EMA): [   0/14]  Time: 0.776 (0.776)  Loss:  6.7930 (6.7930)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.154)  Loss:  6.7930 (6.7905)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.6463)\n",
            "Train: 30 [   0/28 (  0%)]  Loss: 4.074 (4.07)  Time: 1.383s,   92.57/s  (1.383s,   92.57/s)  LR: 9.758e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.004 (1.004)\n",
            "Train: 30 [  27/28 (100%)]  Loss: 3.518 (3.82)  Time: 0.366s,  349.34/s  (0.416s,  308.02/s)  LR: 9.758e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.048)\n",
            "Test: [   0/14]  Time: 0.780 (0.780)  Loss:  3.0254 (3.0254)  Acc@1: 20.3125 (20.3125)  Acc@5: 52.3438 (52.3438)\n",
            "Test: [  14/14]  Time: 0.026 (0.155)  Loss:  2.9258 (2.5944)  Acc@1: 25.0000 (29.3122)  Acc@5: 60.0000 (67.1943)\n",
            "Test (EMA): [   0/14]  Time: 0.708 (0.708)  Loss:  6.7812 (6.7812)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.150)  Loss:  6.7852 (6.7808)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.6463)\n",
            "Train: 31 [   0/28 (  0%)]  Loss: 3.828 (3.83)  Time: 1.131s,  113.19/s  (1.131s,  113.19/s)  LR: 9.741e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.753 (0.753)\n",
            "Train: 31 [  27/28 (100%)]  Loss: 3.673 (3.81)  Time: 0.367s,  348.41/s  (0.407s,  314.14/s)  LR: 9.741e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.040)\n",
            "Test: [   0/14]  Time: 0.791 (0.791)  Loss:  3.4141 (3.4141)  Acc@1:  2.3438 ( 2.3438)  Acc@5: 38.2812 (38.2812)\n",
            "Test: [  14/14]  Time: 0.026 (0.156)  Loss:  2.9805 (2.6420)  Acc@1: 25.0000 (26.4192)  Acc@5: 55.0000 (64.4105)\n",
            "Test (EMA): [   0/14]  Time: 0.752 (0.752)  Loss:  6.7734 (6.7734)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.153)  Loss:  6.7734 (6.7721)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.6463)\n",
            "Train: 32 [   0/28 (  0%)]  Loss: 3.938 (3.94)  Time: 1.458s,   87.78/s  (1.458s,   87.78/s)  LR: 9.725e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.092 (1.092)\n",
            "Train: 32 [  27/28 (100%)]  Loss: 3.729 (3.79)  Time: 0.368s,  347.60/s  (0.418s,  306.10/s)  LR: 9.725e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.052)\n",
            "Test: [   0/14]  Time: 0.737 (0.737)  Loss:  2.8281 (2.8281)  Acc@1: 26.5625 (26.5625)  Acc@5: 57.8125 (57.8125)\n",
            "Test: [  14/14]  Time: 0.025 (0.152)  Loss:  2.5938 (2.6316)  Acc@1: 25.0000 (28.8210)  Acc@5: 67.5000 (66.1026)\n",
            "Test (EMA): [   0/14]  Time: 0.726 (0.726)  Loss:  6.7617 (6.7617)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.151)  Loss:  6.7656 (6.7613)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.7009)\n",
            "Train: 33 [   0/28 (  0%)]  Loss: 3.469 (3.47)  Time: 1.434s,   89.27/s  (1.434s,   89.27/s)  LR: 9.707e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.059 (1.059)\n",
            "Train: 33 [  27/28 (100%)]  Loss: 3.418 (3.66)  Time: 0.367s,  348.99/s  (0.417s,  306.67/s)  LR: 9.707e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.050)\n",
            "Test: [   0/14]  Time: 0.544 (0.544)  Loss:  3.2070 (3.2070)  Acc@1: 10.1562 (10.1562)  Acc@5: 50.7812 (50.7812)\n",
            "Test: [  14/14]  Time: 0.026 (0.145)  Loss:  3.2422 (2.5962)  Acc@1: 10.0000 (27.6747)  Acc@5: 42.5000 (66.4301)\n",
            "Test (EMA): [   0/14]  Time: 0.765 (0.765)  Loss:  6.7539 (6.7539)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.7539 (6.7506)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.7009)\n",
            "Train: 34 [   0/28 (  0%)]  Loss: 3.884 (3.88)  Time: 1.054s,  121.43/s  (1.054s,  121.43/s)  LR: 9.690e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.676 (0.676)\n",
            "Train: 34 [  27/28 (100%)]  Loss: 3.415 (3.74)  Time: 0.367s,  348.51/s  (0.404s,  317.00/s)  LR: 9.690e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.037)\n",
            "Test: [   0/14]  Time: 0.692 (0.692)  Loss:  3.2051 (3.2051)  Acc@1: 13.2812 (13.2812)  Acc@5: 51.5625 (51.5625)\n",
            "Test: [  14/14]  Time: 0.026 (0.149)  Loss:  2.3438 (2.5873)  Acc@1: 35.0000 (29.6397)  Acc@5: 72.5000 (65.5022)\n",
            "Test (EMA): [   0/14]  Time: 0.769 (0.769)  Loss:  6.7422 (6.7422)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.7422 (6.7403)  Acc@1:  0.0000 ( 2.5655)  Acc@5:  0.0000 (13.7009)\n",
            "Train: 35 [   0/28 (  0%)]  Loss: 3.933 (3.93)  Time: 1.448s,   88.40/s  (1.448s,   88.40/s)  LR: 9.671e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.052 (1.052)\n",
            "Train: 35 [  27/28 (100%)]  Loss: 4.056 (3.73)  Time: 0.368s,  348.28/s  (0.418s,  306.15/s)  LR: 9.671e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.051)\n",
            "Test: [   0/14]  Time: 0.575 (0.575)  Loss:  3.0039 (3.0039)  Acc@1: 17.9688 (17.9688)  Acc@5: 55.4688 (55.4688)\n",
            "Test: [  14/14]  Time: 0.026 (0.146)  Loss:  2.6875 (2.4992)  Acc@1: 27.5000 (30.4039)  Acc@5: 60.0000 (68.9410)\n",
            "Test (EMA): [   0/14]  Time: 0.725 (0.725)  Loss:  6.7305 (6.7305)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.151)  Loss:  6.7305 (6.7283)  Acc@1:  0.0000 ( 2.2380)  Acc@5:  0.0000 (13.6463)\n",
            "Train: 36 [   0/28 (  0%)]  Loss: 3.442 (3.44)  Time: 1.449s,   88.33/s  (1.449s,   88.33/s)  LR: 9.652e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.079 (1.079)\n",
            "Train: 36 [  27/28 (100%)]  Loss: 3.835 (3.74)  Time: 0.368s,  348.00/s  (0.418s,  306.26/s)  LR: 9.652e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.051)\n",
            "Test: [   0/14]  Time: 0.692 (0.692)  Loss:  2.9668 (2.9668)  Acc@1: 17.1875 (17.1875)  Acc@5: 53.9062 (53.9062)\n",
            "Test: [  14/14]  Time: 0.026 (0.150)  Loss:  2.3750 (2.4617)  Acc@1: 40.0000 (32.4782)  Acc@5: 70.0000 (68.6135)\n",
            "Test (EMA): [   0/14]  Time: 0.772 (0.772)  Loss:  6.7188 (6.7188)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.154)  Loss:  6.7148 (6.7168)  Acc@1:  0.0000 ( 2.0742)  Acc@5:  0.0000 (13.8100)\n",
            "Train: 37 [   0/28 (  0%)]  Loss: 3.337 (3.34)  Time: 1.495s,   85.60/s  (1.495s,   85.60/s)  LR: 9.633e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.131 (1.131)\n",
            "Train: 37 [  27/28 (100%)]  Loss: 3.548 (3.66)  Time: 0.370s,  346.21/s  (0.419s,  305.28/s)  LR: 9.633e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.053)\n",
            "Test: [   0/14]  Time: 0.769 (0.769)  Loss:  2.5645 (2.5645)  Acc@1: 25.0000 (25.0000)  Acc@5: 76.5625 (76.5625)\n",
            "Test: [  14/14]  Time: 0.026 (0.154)  Loss:  3.2969 (2.4578)  Acc@1: 12.5000 (32.4782)  Acc@5: 40.0000 (69.1594)\n",
            "Test (EMA): [   0/14]  Time: 0.705 (0.705)  Loss:  6.7070 (6.7070)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.026 (0.150)  Loss:  6.7031 (6.7048)  Acc@1:  0.0000 ( 2.6201)  Acc@5:  0.0000 (13.8646)\n",
            "Train: 38 [   0/28 (  0%)]  Loss: 3.607 (3.61)  Time: 1.065s,  120.23/s  (1.065s,  120.23/s)  LR: 9.613e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.687 (0.687)\n",
            "Train: 38 [  27/28 (100%)]  Loss: 3.599 (3.74)  Time: 0.367s,  349.06/s  (0.408s,  313.72/s)  LR: 9.613e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.040)\n",
            "Test: [   0/14]  Time: 0.547 (0.547)  Loss:  2.6602 (2.6602)  Acc@1: 27.3438 (27.3438)  Acc@5: 68.7500 (68.7500)\n",
            "Test: [  14/14]  Time: 0.026 (0.140)  Loss:  2.8906 (2.3676)  Acc@1: 27.5000 (33.8974)  Acc@5: 57.5000 (72.6528)\n",
            "Test (EMA): [   0/14]  Time: 0.522 (0.522)  Loss:  6.6953 (6.6953)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)\n",
            "Test (EMA): [  14/14]  Time: 0.025 (0.146)  Loss:  6.6914 (6.6925)  Acc@1:  0.0000 ( 2.6747)  Acc@5:  0.0000 (14.0284)\n",
            "Train: 39 [   0/28 (  0%)]  Loss: 3.716 (3.72)  Time: 1.501s,   85.28/s  (1.501s,   85.28/s)  LR: 9.593e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 1.133 (1.133)\n",
            "Train: 39 [  27/28 (100%)]  Loss: 4.009 (3.70)  Time: 0.368s,  348.17/s  (0.420s,  304.47/s)  LR: 9.593e-04, WD0: 0.000000e+00, WD1: 5.000000e-02    Data: 0.000 (0.053)\n",
            "*** Best metric: 3.002183406113537 (epoch 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(teacher_model)"
      ],
      "metadata": {
        "id": "2m9wPp0eXb-i",
        "outputId": "3d1fb4cf-be0f-48ae-e823-85be2676ab3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2m9wPp0eXb-i",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastViT(\n",
            "  (patch_embed): Sequential(\n",
            "    (0): MobileOneBlock(\n",
            "      (se): Identity()\n",
            "      (activation): GELU(approximate='none')\n",
            "      (rbr_conv): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (rbr_scale): Sequential(\n",
            "        (conv): Conv2d(3, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): MobileOneBlock(\n",
            "      (se): Identity()\n",
            "      (activation): GELU(approximate='none')\n",
            "      (rbr_conv): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
            "          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (rbr_scale): Sequential(\n",
            "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(2, 2), groups=48, bias=False)\n",
            "        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): MobileOneBlock(\n",
            "      (se): Identity()\n",
            "      (activation): GELU(approximate='none')\n",
            "      (rbr_skip): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (rbr_conv): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (network): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
            "                (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
            "              (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48, bias=False)\n",
            "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
            "                (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)\n",
            "              (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48, bias=False)\n",
            "            (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (1): PatchEmbed(\n",
            "      (proj): Sequential(\n",
            "        (0): ReparamLargeKernelConv(\n",
            "          (activation): GELU(approximate='none')\n",
            "          (lkb_origin): Sequential(\n",
            "            (conv): Conv2d(48, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)\n",
            "            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (small_conv): Sequential(\n",
            "            (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
            "            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): MobileOneBlock(\n",
            "          (se): Identity()\n",
            "          (activation): GELU(approximate='none')\n",
            "          (rbr_skip): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (rbr_conv): ModuleList(\n",
            "            (0): Sequential(\n",
            "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
            "                (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
            "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
            "            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)\n",
            "                (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), groups=96, bias=False)\n",
            "              (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
            "            (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (3): PatchEmbed(\n",
            "      (proj): Sequential(\n",
            "        (0): ReparamLargeKernelConv(\n",
            "          (activation): GELU(approximate='none')\n",
            "          (lkb_origin): Sequential(\n",
            "            (conv): Conv2d(96, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)\n",
            "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (small_conv): Sequential(\n",
            "            (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): MobileOneBlock(\n",
            "          (se): Identity()\n",
            "          (activation): GELU(approximate='none')\n",
            "          (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (rbr_conv): ModuleList(\n",
            "            (0): Sequential(\n",
            "              (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
            "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
            "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (2): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
            "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (3): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), groups=192, bias=False)\n",
            "              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "            (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "    (5): PatchEmbed(\n",
            "      (proj): Sequential(\n",
            "        (0): ReparamLargeKernelConv(\n",
            "          (activation): GELU(approximate='none')\n",
            "          (lkb_origin): Sequential(\n",
            "            (conv): Conv2d(192, 384, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)\n",
            "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (small_conv): Sequential(\n",
            "            (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
            "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): MobileOneBlock(\n",
            "          (se): Identity()\n",
            "          (activation): GELU(approximate='none')\n",
            "          (rbr_skip): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (rbr_conv): ModuleList(\n",
            "            (0): Sequential(\n",
            "              (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
            "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "      (1): RepMixerBlock(\n",
            "        (token_mixer): RepMixer(\n",
            "          (norm): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (mixer): MobileOneBlock(\n",
            "            (se): Identity()\n",
            "            (activation): Identity()\n",
            "            (rbr_skip): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (rbr_conv): ModuleList(\n",
            "              (0): Sequential(\n",
            "                (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "                (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              )\n",
            "            )\n",
            "            (rbr_scale): Sequential(\n",
            "              (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
            "              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (convffn): ConvFFN(\n",
            "          (conv): Sequential(\n",
            "            (conv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "            (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "          (fc1): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
            "  (conv_exp): MobileOneBlock(\n",
            "    (se): SEBlock(\n",
            "      (reduce): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (activation): GELU(approximate='none')\n",
            "    (rbr_conv): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (rbr_scale): Sequential(\n",
            "      (conv): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=384, bias=False)\n",
            "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_teacher = create_model(\n",
        "    \"fastvit_t8\",\n",
        "    pretrained=False,\n",
        "    num_classes=37,\n",
        ")\n",
        "\n",
        "checkpoint = torch.load(\"/content/fastvit-pet-mobile/output/teacher/20251119-225323-fastvit_t8-256/model_best.pth.tar\", map_location='cpu')\n",
        "tuned_teacher_model.load_state_dict(checkpoint['state_dict'])\n"
      ],
      "metadata": {
        "id": "qrRxpjRsZPgb",
        "outputId": "7ddec511-ec68-4fe2-b212-50142a75465c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        }
      },
      "id": "qrRxpjRsZPgb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1899301985.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/fastvit-pet-mobile/output/teacher/20251119-225323-fastvit_t8-256/model_best.pth.tar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtuned_teacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1527\u001b[0m                         )\n\u001b[1;32m   1528\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 return _load(\n\u001b[1;32m   1531\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986cb1d7",
      "metadata": {
        "id": "986cb1d7"
      },
      "source": [
        "## 4. Setup and finetune student model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8345ff9b",
      "metadata": {
        "id": "8345ff9b"
      },
      "source": [
        "## 5. Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40700cd",
      "metadata": {
        "id": "f40700cd"
      },
      "source": [
        "## 6. Export model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "293d835c",
      "metadata": {
        "id": "293d835c"
      },
      "source": [
        "## 7. Summary and conclusion"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}